% References for SOKRATES paper

% Chain-of-Thought
@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

% PrOntoQA
@inproceedings{saparov2023language,
  title={Language models are greedy reasoners: A systematic formal analysis of chain-of-thought},
  author={Saparov, Abulhair and He, He},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% FOLIO
@inproceedings{han2022folio,
  title={{FOLIO}: Natural language reasoning with first-order logic},
  author={Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and others},
  booktitle={arXiv preprint arXiv:2209.00840},
  year={2022}
}

% DPO
@inproceedings{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

% Options Framework
@article{sutton1999between,
  title={Between {MDPs} and semi-{MDPs}: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial Intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

% OaK / Reward-Respecting Subtasks
@article{sutton2023reward,
  title={Reward-respecting subtasks for model-based reinforcement learning},
  author={Sutton, Richard S and Machado, Marlos C and Holland, G Zacharias and Szepesvari, David and Timbers, Finbarr and Tanner, Brian and White, Adam},
  journal={Artificial Intelligence},
  volume={324},
  pages={104001},
  year={2023},
  publisher={Elsevier}
}

% LoCo-LMs
@inproceedings{riegel2020logical,
  title={Logical neural networks},
  author={Riegel, Ryan and Gray, Alexander and Luus, Francois and Khan, Naweed and Makondo, Ndivhuwo and Akhalwaya, Ismail Yunus and Qian, Haifeng and Faber, Ronald and Baez, Edward and Srivastava, Shyam and others},
  booktitle={arXiv preprint arXiv:2006.13155},
  year={2020}
}

% Logic-LM
@inproceedings{pan2023logic,
  title={Logic-{LM}: Empowering large language models with symbolic solvers for faithful logical reasoning},
  author={Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William Yang},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={3806--3824},
  year={2023}
}

% LINC
@inproceedings{olausson2023linc,
  title={{LINC}: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers},
  author={Olausson, Theo X and Gu, Alex and Lipkin, Benjamin and Zhang, Cedegao E and Solar-Lezama, Armando and Tenenbaum, Joshua B and Levy, Roger},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5153--5176},
  year={2023}
}

% LAMBADA
@inproceedings{kazemi2022lambada,
  title={{LAMBADA}: Backward chaining for automated reasoning in natural language},
  author={Kazemi, Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
  booktitle={arXiv preprint arXiv:2212.13894},
  year={2022}
}

% RLHF / InstructGPT
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

% LoRA
@inproceedings{hu2022lora,
  title={{LoRA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

% ReAct
@inproceedings{yao2023react,
  title={{ReAct}: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% Qwen2
@article{qwen2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

% LoGiPT
@inproceedings{feng2024logipt,
  title={{LoGiPT}: Teaching language models to reason logically by learning from proof graphs},
  author={Feng, Jianfeng and Zhang, Zhengda and Wu, Shimin and Ding, Ming and Sui, Zhihua and Zhou, Jie},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2024}
}

% VeriCoT
@article{ling2023deductive,
  title={Deductive verification of chain-of-thought reasoning},
  author={Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Minhao and Memez, Rithwick and Wu, Kai and Zhu, Jiayi and Wu, Jiaxing and Tang, Hao and others},
  journal={arXiv preprint arXiv:2306.03872},
  year={2023}
}

% Self-Consistency
@inproceedings{wang2023selfconsistency,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% Cannot Self-Correct
@inproceedings{huang2024large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Gu, Shima Sam and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

% RuleTaker / ProofWriter
@inproceedings{clark2021transformers,
  title={Transformers as soft reasoners over language},
  author={Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  booktitle={International Joint Conference on Artificial Intelligence},
  pages={3882--3890},
  year={2021}
}

% Tree of Thoughts
@inproceedings{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% Buffer of Thoughts
@article{yang2024buffer,
  title={Buffer of thoughts: Thought-augmented reasoning with large language models},
  author={Yang, Ling and Yu, Zhaochen and Zhang, Tianjun and Cao, Shiyi and Xu, Minkai and Zhang, Wentao and Gonzalez, Joseph E and Cui, Bin},
  journal={arXiv preprint arXiv:2406.04271},
  year={2024}
}

